{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import re\n",
    "import os\n",
    "from sklearn import *\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dataset): # Получение расположения файлов\n",
    "    files_type = ['train', 'test']\n",
    "    if (dataset in files_type):\n",
    "        conn = pg.connect('dbname=articles user=postgres password=1234') # Подключение к БД\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f'SELECT link, class_id FROM {dataset};') # Извлечение из БД ссылок на статьи и разметки классов\n",
    "        files = cur.fetchall()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return files\n",
    "    else:\n",
    "        print('Указан неверный dataset')\n",
    "\n",
    "def load_data(files): # Загрузка данных из файлов\n",
    "    snowball = SnowballStemmer(language='russian')\n",
    "    regex = re.compile('([А-Яа-я]{2,100})')\n",
    "    content = [] \n",
    "    for i in range(len(files)):\n",
    "        words = list(map(str.lower, regex.findall(textract.process(files[i][0]).decode())))\n",
    "        if(len(words) != 0):\n",
    "            words = list(filter(lambda word: word not in stopwords.words('russian'), words))\n",
    "            words = list(map(snowball.stem, words))\n",
    "            content.append((str.join(' ', words), files[i][1]))\n",
    "    return content\n",
    "\n",
    "def val_split(dataset, frac): # Разделение на валидационный набор\n",
    "    val_split = np.unique([content[1] for content in dataset])\n",
    "    val_content = []\n",
    "    for label in val_split:\n",
    "        indices = []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][1] == label:\n",
    "                indices.append(i)\n",
    "        [val_content.append(dataset.pop(index)) for index in [indices[::frac][j]-j for j in range(len(indices[::frac]))]]\n",
    "    return val_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_files = load_files('train')\n",
    "# train_content = load_data(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Размер обучающих данных: {len(train_content)} экземпляров')\n",
    "print(train_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = load_files('test')            \n",
    "test_content = load_data(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Размер тестовых данных: {len(test_content)} экземпляров')\n",
    "print(test_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "vectorizer.fit([data[0] for data in train_content])\n",
    "\n",
    "x_train = vectorizer.transform([data[0] for data in train_content]).toarray()\n",
    "x_test = []\n",
    "\n",
    "for data in test_content:\n",
    "    x_test.append(vectorizer.transform([data[0]]).toarray()[0])\n",
    "    \n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data[1] for data in train_content]\n",
    "y_test = [data[1] for data in test_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive Bayes\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "nb = naive_bayes.GaussianNB()\n",
    "nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# KNN\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Decision Tree\n",
    "\n",
    "tree = sklearn.tree.DecisionTreeClassifier(criterion='log_loss')\n",
    "tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SVM\n",
    "\n",
    "svm = sklearn.svm.SVC(kernel='linear', probability=True, cache_size=2000)\n",
    "svm.fit(x_train, y_train)\n",
    "svm.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svm = sklearn.svm.SVC(kernel='rbf', cache_size=2000)\n",
    "svm.fit(x_train, y_train)\n",
    "svm.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = sklearn.svm.SVC(kernel='sigmoid', cache_size=1000)\n",
    "svm.fit(x_train, y_train)\n",
    "svm.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random forest\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=150, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bagging SVM\n",
    "\n",
    "base = sklearn.svm.SVC(kernel='linear')\n",
    "bag = sklearn.ensemble.BaggingClassifier(base_estimator=base, n_estimators=10, max_samples=0.5, max_features=0.5, n_jobs=-1)\n",
    "bag.fit(x_train, y_train)\n",
    "bag.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging NB\n",
    "\n",
    "base = sklearn.naive_bayes.GaussianNB()\n",
    "bag = sklearn.ensemble.BaggingClassifier(base_estimator=base, n_estimators=10, max_samples=0.5, max_features=0.5, n_jobs=-1)\n",
    "bag.fit(x_train, y_train)\n",
    "bag.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS NN\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "vect = TextVectorization(output_mode='tf_idf')#, max_tokens=max_features)\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    vect.adapt([data[0] for data in train_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_content = val_split(train_content, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vect([data[0] for data in train_content]).numpy()\n",
    "y_train = np.array([data[1] for data in train_content])#.reshape(-1,1)\n",
    "y_train = tf.one_hot(y_train, len(np.unique(y_train))).numpy()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vect([data[0] for data in test_content]).numpy()\n",
    "y_test = np.array([data[1] for data in test_content])#.reshape(-1,1)\n",
    "y_test = tf.one_hot(y_test, len(np.unique(y_test))).numpy()\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Вектор слов:')\n",
    "print(x_train)\n",
    "print('Вектор классов:')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = vect([data[0] for data in val_content]).numpy()\n",
    "y_val = np.array([data[1] for data in val_content])#.reshape(-1,1)\n",
    "y_val = tf.one_hot(y_val, len(np.unique(y_val))).numpy()\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(np.unique([c[1] for c in train_content]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(400, activation=\"relu\"),\n",
    "            layers.Dense(300, activation=\"relu\"),\n",
    "            layers.Dense(200, activation=\"relu\"),\n",
    "            layers.Dense(num_labels, activation=\"softmax\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=x_train, y=y_train, batch_size=None, validation_data=(x_val, y_val), epochs=epochs)\n",
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, categorical_acc = model.evaluate(x=x_test, y=y_test)\n",
    "print(f'Функция ошибки на тестовых данных: {loss}')\n",
    "print(f'Точность на тестовых данных: {categorical_acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm.predict(x_test)\n",
    "comparison = np.array([[pred[i], test_content[i][1]] for i in range(len(pred))])\n",
    "accuracy = round(sum([1 if p[0] == p[1] else 0 for p in comparison])/len(comparison), 2)\n",
    "\n",
    "pred_labels = [list(filter(lambda p_: p_[0]==label, comparison)) for label in np.unique(comparison)]\n",
    "target_labels = [list(filter(lambda p_: p_[1]==label, comparison)) for label in np.unique(comparison)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision = [round(sum([1 if p[0] == p[1] else 0 for p in pred_labels[label]])/len(pred_labels[label]), 2) for label in np.unique(comparison)]\n",
    "macro_recall = [round(sum([1 if p[0] == p[1] else 0 for p in target_labels[label]])/len(target_labels[label]), 2) for label in np.unique(comparison)]\n",
    "print('===Macro-average===\\n')\n",
    "print('precision:\\t', ';\\t'.join('Class {0}: {1}'.format(*p) for p in enumerate(macro_precision)))\n",
    "print('recall: \\t', ';\\t'.join('Class {0}: {1}'.format(*p) for p in enumerate(macro_recall)))\n",
    "print('F-score:\\t', ';\\t'.join('Class {0}: {1}'.format(*p) for p in enumerate(macro_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg.connect('dbname=articles user=postgres password=1234')\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT * FROM classes;')\n",
    "section_map = {}\n",
    "for section in cur.fetchall():\n",
    "    section_map[section[0]] = section[1]\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_articles = load_data([test_files[12], test_files[96], test_files[182]])\n",
    "names = []\n",
    "for article in [test_files[12], test_files[96], test_files[182]]:\n",
    "    names.append(f'\\n{article[0].split(\"/\")[-1]}')\n",
    "    print(f'Класс: {article[1]} - {section_map[article[1]]}\\nПуть к файлу: {article[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec.transform([data[0] for data in test_articles]).toarray()\n",
    "for i in range(len(X)):\n",
    "    print(names[i])\n",
    "    prediction = svm.predict_proba(X[i].reshape(1,-1))\n",
    "    prediction = [round(proba, 2) for proba in prediction.reshape(-1,)]\n",
    "    for j in range(len(prediction)):\n",
    "        print(f'Класс {j} - {section_map[j]}: {prediction[j]*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
